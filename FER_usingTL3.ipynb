{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmT-R20PQkBm"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkTpv2_hWSCu"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import cv2\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtzaKqvlmeDQ"
      },
      "outputs": [],
      "source": [
        "img_array = cv2.imread(\"/content/drive/MyDrive/FER2013/train/0/Training_3908.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quK7FsWDm6hm"
      },
      "outputs": [],
      "source": [
        "img_array.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9HHdFZ3nPJc"
      },
      "outputs": [],
      "source": [
        "plt.imshow(img_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ilafnGn1nSyk"
      },
      "outputs": [],
      "source": [
        "datadirectory = \"/content/drive/MyDrive/FER2013/train/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIHiucwznbo8"
      },
      "outputs": [],
      "source": [
        "classes = [\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\"]   ## LIST OF CLASSES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q203YWWUnjDV"
      },
      "outputs": [],
      "source": [
        "for category in classes:\n",
        "  path = os.path.join(datadirectory , category)\n",
        "  for img in os.listdir(path):\n",
        "    img_array = cv2.imread(os.path.join(path, img))\n",
        "    plt.imshow(cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB))\n",
        "    plt.show()\n",
        "    break\n",
        "  break  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seE_o_n6oIKc"
      },
      "outputs": [],
      "source": [
        "img_size = 224    #CHANGED THE SIZE FROM 48 X 48 TO 224 X 224 WHICH IS ACCEPTED BY TRANSFER LEARNING MODEL \n",
        "new_array = cv2.resize(img_array,(img_size, img_size))\n",
        "plt.imshow(cv2.cvtColor(new_array, cv2.COLOR_BGR2RGB))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KdglXQ2spmqT"
      },
      "outputs": [],
      "source": [
        "new_array.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6fXkk5zyJKq"
      },
      "source": [
        "NOW READING ALL THE IMAGES AND CONVERTING THEM TO ARRAY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gFwqR8Ypr40"
      },
      "outputs": [],
      "source": [
        "training_data = []\n",
        "def create_training_Data():\n",
        "  for category in classes:\n",
        "    path = os.path.join(datadirectory , category)\n",
        "    class_num = classes.index(category)   #Labeling or indexing\n",
        "    for img in os.listdir(path):\n",
        "       try:\n",
        "            img_array = cv2.imread(os.path.join(path, img))\n",
        "            new_array = cv2.resize(img_array,(img_size , img_size))\n",
        "            training_data.append([new_array , class_num])\n",
        "       except Exception as e:\n",
        "            pass  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ca5Sz1eOreZQ"
      },
      "outputs": [],
      "source": [
        "create_training_Data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5iCF7qy2veqF"
      },
      "outputs": [],
      "source": [
        "print(len(training_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJbNgl2_wNF7"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "random.shuffle(training_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdMkgNZSObgz"
      },
      "outputs": [],
      "source": [
        "print(len((training_data)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-M5iHA2wZsS"
      },
      "outputs": [],
      "source": [
        "x = []  # data / feature\n",
        "y = [] #label / numbering\n",
        "\n",
        "for features,label in training_data:\n",
        "  x.append(features)\n",
        "  y.append(label)\n",
        "\n",
        "x = np.array(x).reshape(-1, img_size, img_size, 3)  ## coverting x array to 4 Dimension array. because we are USING deep learning architecture such as mobileNet which takes 4D images \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DuRoX49GN4SD"
      },
      "outputs": [],
      "source": [
        "type(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFHr489jw_yz"
      },
      "outputs": [],
      "source": [
        "x.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGCMa3Jlzj2n"
      },
      "outputs": [],
      "source": [
        "X= np.array(x)\n",
        "Y= np.array(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1XwF41Hzv3-"
      },
      "source": [
        "#DEEP LEARNING MODEL FOR TRAINING - TRANSFER LEARNING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60IWisXRz7KL"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Flatten\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9sTWfQT40sXA"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.applications.MobileNetV2()     ##mobileNet is a pre-trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yh3bxmOD18ee"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mTVQmxa2l-8"
      },
      "source": [
        "# TRANSFER LEARNING - TUNING , WEIGHTS WILL START FROM LAST CHECK POINT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wckP34t2TEe"
      },
      "outputs": [],
      "source": [
        "base_input = model.layers[0].input    ##input layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80aaTZmq24gq"
      },
      "outputs": [],
      "source": [
        "base_output = model.layers[-2].output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTP_VXlE3veU"
      },
      "outputs": [],
      "source": [
        "base_output   ##last row was deleted, coz they were unneccessary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3eROD_vh3AsR"
      },
      "outputs": [],
      "source": [
        "final_output = layers.Dense(128)(base_output)   ## global pooling layer.\n",
        "## adding new layer after the output of global pooling layer.\n",
        "final_output = layers.Activation('relu')(final_output)\n",
        "final_output = layers.Dense(64)(final_output)\n",
        "final_output = layers.Activation('relu')(final_output)\n",
        "final_output = layers.Dense(7,activation='softmax')(final_output)   ## Classification layer\n",
        "    ## output layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qi-3YiHY3QF7"
      },
      "outputs": [],
      "source": [
        "final_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlviZ2lrJQfk"
      },
      "outputs": [],
      "source": [
        "new_model = keras.Model(inputs = base_input , outputs = final_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXTT8oqUJAK2"
      },
      "outputs": [],
      "source": [
        "new_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ysjO1bcJNql"
      },
      "outputs": [],
      "source": [
        "new_model.compile(optimizer='adam',loss='kullback_leibler_divergence',metrics = ['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YI7SZ79AKTXB",
        "outputId": "42e3d2d8-29fd-4198-e4a0-bfa2aead05a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "332/332 [==============================] - 87s 209ms/step - loss: 0.8566 - accuracy: 0.6385 - val_loss: 3.8493 - val_accuracy: 0.4414\n",
            "Epoch 2/50\n",
            "332/332 [==============================] - 69s 208ms/step - loss: 0.6828 - accuracy: 0.7206 - val_loss: 2.5898 - val_accuracy: 0.4160\n",
            "Epoch 3/50\n",
            "332/332 [==============================] - 69s 208ms/step - loss: 0.6006 - accuracy: 0.7569 - val_loss: 1.6735 - val_accuracy: 0.5518\n",
            "Epoch 4/50\n",
            "332/332 [==============================] - 69s 208ms/step - loss: 0.5445 - accuracy: 0.7771 - val_loss: 7.0916 - val_accuracy: 0.3039\n",
            "Epoch 5/50\n",
            "332/332 [==============================] - 69s 208ms/step - loss: 0.4889 - accuracy: 0.8064 - val_loss: 4.3051 - val_accuracy: 0.3650\n",
            "Epoch 6/50\n",
            "332/332 [==============================] - 69s 208ms/step - loss: 0.4551 - accuracy: 0.8190 - val_loss: 1.8540 - val_accuracy: 0.5424\n",
            "Epoch 7/50\n",
            "332/332 [==============================] - 69s 208ms/step - loss: 0.4145 - accuracy: 0.8329 - val_loss: 1.8683 - val_accuracy: 0.5713\n",
            "Epoch 8/50\n",
            "332/332 [==============================] - 69s 208ms/step - loss: 0.3575 - accuracy: 0.8570 - val_loss: 2.8357 - val_accuracy: 0.5552\n",
            "Epoch 9/50\n",
            "332/332 [==============================] - 69s 209ms/step - loss: 0.3373 - accuracy: 0.8684 - val_loss: 1.0819 - val_accuracy: 0.5739\n",
            "Epoch 10/50\n",
            "332/332 [==============================] - 69s 208ms/step - loss: 0.2895 - accuracy: 0.8877 - val_loss: 2.7226 - val_accuracy: 0.5458\n",
            "Epoch 11/50\n",
            "332/332 [==============================] - 69s 209ms/step - loss: 0.2595 - accuracy: 0.9035 - val_loss: 3.9413 - val_accuracy: 0.5034\n",
            "Epoch 12/50\n",
            "332/332 [==============================] - 69s 208ms/step - loss: 0.2321 - accuracy: 0.9119 - val_loss: 2.0137 - val_accuracy: 0.6460\n",
            "Epoch 13/50\n",
            "332/332 [==============================] - 69s 208ms/step - loss: 0.2160 - accuracy: 0.9197 - val_loss: 6.3002 - val_accuracy: 0.4117\n",
            "Epoch 14/50\n",
            "332/332 [==============================] - 69s 208ms/step - loss: 0.1924 - accuracy: 0.9281 - val_loss: 2.5177 - val_accuracy: 0.5815\n",
            "Epoch 15/50\n",
            "332/332 [==============================] - 69s 209ms/step - loss: 0.1618 - accuracy: 0.9414 - val_loss: 2.1787 - val_accuracy: 0.5968\n",
            "Epoch 16/50\n",
            "332/332 [==============================] - 69s 207ms/step - loss: 0.1409 - accuracy: 0.9486 - val_loss: 4.0031 - val_accuracy: 0.4677\n",
            "Epoch 17/50\n",
            "332/332 [==============================] - 69s 208ms/step - loss: 0.1527 - accuracy: 0.9443 - val_loss: 2.5619 - val_accuracy: 0.5866\n",
            "Epoch 18/50\n",
            "332/332 [==============================] - 69s 209ms/step - loss: 0.1372 - accuracy: 0.9505 - val_loss: 1.4485 - val_accuracy: 0.6766\n",
            "Epoch 19/50\n",
            "332/332 [==============================] - 69s 209ms/step - loss: 0.1140 - accuracy: 0.9595 - val_loss: 1.6406 - val_accuracy: 0.6706\n",
            "Epoch 20/50\n",
            "332/332 [==============================] - 69s 208ms/step - loss: 0.1052 - accuracy: 0.9635 - val_loss: 2.6044 - val_accuracy: 0.5654\n",
            "Epoch 21/50\n",
            "332/332 [==============================] - 69s 208ms/step - loss: 0.1118 - accuracy: 0.9597 - val_loss: 1.9770 - val_accuracy: 0.5951\n",
            "Epoch 22/50\n",
            "332/332 [==============================] - 69s 207ms/step - loss: 0.1113 - accuracy: 0.9598 - val_loss: 1.7415 - val_accuracy: 0.6800\n",
            "Epoch 23/50\n",
            "332/332 [==============================] - 69s 208ms/step - loss: 0.0882 - accuracy: 0.9678 - val_loss: 2.0391 - val_accuracy: 0.6418\n",
            "Epoch 24/50\n",
            "332/332 [==============================] - 69s 208ms/step - loss: 0.0915 - accuracy: 0.9695 - val_loss: 1.7335 - val_accuracy: 0.6019\n",
            "Epoch 25/50\n",
            "332/332 [==============================] - 69s 208ms/step - loss: 0.1003 - accuracy: 0.9657 - val_loss: 1.7647 - val_accuracy: 0.6935\n",
            "Epoch 26/50\n",
            "332/332 [==============================] - 69s 208ms/step - loss: 0.0840 - accuracy: 0.9688 - val_loss: 2.1172 - val_accuracy: 0.6562\n",
            "Epoch 27/50\n",
            "332/332 [==============================] - 70s 210ms/step - loss: 0.0895 - accuracy: 0.9703 - val_loss: 1.6102 - val_accuracy: 0.6647\n",
            "Epoch 28/50\n",
            "332/332 [==============================] - 70s 209ms/step - loss: 0.0890 - accuracy: 0.9676 - val_loss: 2.4556 - val_accuracy: 0.6180\n",
            "Epoch 29/50\n",
            "332/332 [==============================] - 69s 209ms/step - loss: 0.0811 - accuracy: 0.9716 - val_loss: 1.7774 - val_accuracy: 0.6766\n",
            "Epoch 30/50\n",
            "332/332 [==============================] - 69s 209ms/step - loss: 0.0594 - accuracy: 0.9807 - val_loss: 2.3626 - val_accuracy: 0.6307\n",
            "Epoch 31/50\n",
            "332/332 [==============================] - 70s 210ms/step - loss: 0.0831 - accuracy: 0.9703 - val_loss: 2.0473 - val_accuracy: 0.6044\n",
            "Epoch 32/50\n",
            "332/332 [==============================] - 69s 209ms/step - loss: 0.0672 - accuracy: 0.9774 - val_loss: 1.5739 - val_accuracy: 0.6579\n",
            "Epoch 33/50\n",
            "332/332 [==============================] - 70s 210ms/step - loss: 0.0947 - accuracy: 0.9679 - val_loss: 2.3833 - val_accuracy: 0.6503\n",
            "Epoch 34/50\n",
            "332/332 [==============================] - 70s 210ms/step - loss: 0.0639 - accuracy: 0.9782 - val_loss: 2.0080 - val_accuracy: 0.6511\n",
            "Epoch 35/50\n",
            "332/332 [==============================] - 70s 210ms/step - loss: 0.1207 - accuracy: 0.9582 - val_loss: 1.6700 - val_accuracy: 0.6808\n",
            "Epoch 36/50\n",
            "332/332 [==============================] - 70s 210ms/step - loss: 0.0513 - accuracy: 0.9832 - val_loss: 1.8696 - val_accuracy: 0.6596\n",
            "Epoch 37/50\n",
            "332/332 [==============================] - 70s 210ms/step - loss: 0.0653 - accuracy: 0.9762 - val_loss: 1.9488 - val_accuracy: 0.6613\n",
            "Epoch 38/50\n",
            "332/332 [==============================] - 70s 210ms/step - loss: 0.0519 - accuracy: 0.9829 - val_loss: 1.5804 - val_accuracy: 0.7012\n",
            "Epoch 39/50\n",
            "332/332 [==============================] - 70s 209ms/step - loss: 0.0509 - accuracy: 0.9838 - val_loss: 1.5661 - val_accuracy: 0.7216\n",
            "Epoch 40/50\n",
            "332/332 [==============================] - 70s 210ms/step - loss: 0.0652 - accuracy: 0.9768 - val_loss: 1.7151 - val_accuracy: 0.6902\n",
            "Epoch 41/50\n",
            "332/332 [==============================] - 71s 213ms/step - loss: 0.0648 - accuracy: 0.9767 - val_loss: 2.4964 - val_accuracy: 0.6834\n",
            "Epoch 42/50\n",
            "332/332 [==============================] - 70s 209ms/step - loss: 0.0855 - accuracy: 0.9706 - val_loss: 1.7698 - val_accuracy: 0.7037\n",
            "Epoch 43/50\n",
            "332/332 [==============================] - 69s 209ms/step - loss: 0.0516 - accuracy: 0.9828 - val_loss: 2.1083 - val_accuracy: 0.7054\n",
            "Epoch 44/50\n",
            "332/332 [==============================] - 69s 209ms/step - loss: 0.0502 - accuracy: 0.9823 - val_loss: 2.0981 - val_accuracy: 0.6647\n",
            "Epoch 45/50\n",
            "332/332 [==============================] - 70s 210ms/step - loss: 0.0439 - accuracy: 0.9853 - val_loss: 1.8157 - val_accuracy: 0.6969\n",
            "Epoch 46/50\n",
            "332/332 [==============================] - 70s 210ms/step - loss: 0.0431 - accuracy: 0.9858 - val_loss: 1.9760 - val_accuracy: 0.6774\n",
            "Epoch 47/50\n",
            "332/332 [==============================] - 70s 210ms/step - loss: 0.0582 - accuracy: 0.9818 - val_loss: 1.8341 - val_accuracy: 0.7012\n",
            "Epoch 48/50\n",
            "332/332 [==============================] - 70s 210ms/step - loss: 0.0921 - accuracy: 0.9706 - val_loss: 1.9043 - val_accuracy: 0.6774\n",
            "Epoch 49/50\n",
            "332/332 [==============================] - 70s 210ms/step - loss: 0.0444 - accuracy: 0.9845 - val_loss: 1.9370 - val_accuracy: 0.6986\n",
            "Epoch 50/50\n",
            "332/332 [==============================] - 70s 210ms/step - loss: 0.0372 - accuracy: 0.9885 - val_loss: 1.6544 - val_accuracy: 0.7343\n"
          ]
        }
      ],
      "source": [
        "history = new_model.fit(X, Y, batch_size=32, epochs=50, validation_split=0.1) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Ot3yb9X3LCvN"
      },
      "outputs": [],
      "source": [
        "new_model.save('/content/drive/MyDrive/final_model_95p07.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "a8hOHcVKLZfP"
      },
      "outputs": [],
      "source": [
        "new_model = tf.keras.models.load_model('/content/drive/MyDrive/final_model_95p07.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TUi9LahBoou"
      },
      "outputs": [],
      "source": [
        "frame=cv2.imread(\"/content/drive/MyDrive/11.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8AIX-ccDwHH"
      },
      "outputs": [],
      "source": [
        "frame = np.array(frame)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5oyWzx8ADzeF"
      },
      "outputs": [],
      "source": [
        "frame.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7IQknD_D1u2"
      },
      "outputs": [],
      "source": [
        "plt.imshow(cv2.cvtColor(frame,cv2.COLOR_BGR2RGB))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xoc0hJcKD4G2"
      },
      "outputs": [],
      "source": [
        "faceCascade= cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmTm_JjcEVnx"
      },
      "outputs": [],
      "source": [
        "gray =cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9hqImQeEZEE"
      },
      "outputs": [],
      "source": [
        "gray.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPcmA3w3Edvz"
      },
      "outputs": [],
      "source": [
        "faces =faceCascade.detectMultiScale(gray,1.1,4)  ##open cv code, find all possible faces in a photo, and can also detect multiple faces\n",
        "for x,y,w,h in faces:\n",
        "    roi_gray =gray[y:y+h,x:x+w]\n",
        "    roi_color =frame[y:y+h, x:x+w]\n",
        "    cv2.rectangle(frame, (x,y), (x+w,y+h), (225,0,0), 2)\n",
        "    facess=faceCascade.detectMultiScale(roi_gray)\n",
        "    if len(faces) ==0:\n",
        "        print(\"Face not detected\")\n",
        "    else:\n",
        "        for (ex,ey,ew,eh) in facess:\n",
        "            face_roi =roi_color[ey: ey+eh, ex:ex +ew]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bf7e4YxiEitY"
      },
      "outputs": [],
      "source": [
        "plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTlQYRreEpxW"
      },
      "outputs": [],
      "source": [
        "plt.imshow(cv2.cvtColor(face_roi, cv2.COLOR_BGR2RGB))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3tLU-iUEut0"
      },
      "outputs": [],
      "source": [
        "final_image=cv2.resize(face_roi,(224,224))  ##face_roi is rgb\n",
        "final_image =np.expand_dims(final_image,axis=0)  #need 4th dimensions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3VbcWtrE08T"
      },
      "outputs": [],
      "source": [
        "Predictions=new_model.predict(final_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQdCRF0ME57b"
      },
      "outputs": [],
      "source": [
        "Predictions[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0oxd6yODwS3"
      },
      "outputs": [],
      "source": [
        "type(Predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5-mzMgdD7zl"
      },
      "outputs": [],
      "source": [
        "Predictions.shape"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}